CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2630 v2 @ 2.60GHz
    Hardware threads: 12
    Total Memory: 264106564 kB
-------------------------------------------------------------------
=== Running /home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv_ndl_deprecated.cntk currentDirectory=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData RunDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu DataDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv OutputDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu DeviceId=0 timestamping=true Train=[SGD=[maxEpochs=5]] Train=[SGD=[epochSize=100]] stderr=-
CNTK 2.1+ (HEAD 572609, Aug 21 2017 08:22:49) on ad5c3ab446a4 at 2017/08/21 08:51:33

/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/build/gpu/release/bin/cntk  configFile=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv_ndl_deprecated.cntk  currentDirectory=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData  RunDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu  DataDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData  ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv  OutputDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu  DeviceId=0  timestamping=true  Train=[SGD=[maxEpochs=5]]  Train=[SGD=[epochSize=100]]  stderr=-
Changed current directory to /tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData
08/21/2017 08:51:33: Redirecting stderr to file -_Train_Test.log
08/21/2017 08:51:33: -------------------------------------------------------------------
08/21/2017 08:51:33: Build info: 

08/21/2017 08:51:33: 		Built time: Aug 21 2017 08:20:29
08/21/2017 08:51:33: 		Last modified date: Thu Aug  3 09:47:36 2017
08/21/2017 08:51:33: 		Build type: release
08/21/2017 08:51:33: 		Build target: GPU
08/21/2017 08:51:33: 		With 1bit-SGD: no
08/21/2017 08:51:33: 		With ASGD: yes
08/21/2017 08:51:33: 		Math lib: mkl
08/21/2017 08:51:33: 		CUDA_PATH: /usr/local/cuda-8.0
08/21/2017 08:51:33: 		CUB_PATH: /usr/local/cub-1.4.1
08/21/2017 08:51:33: 		CUDNN_PATH: /usr/local/cudnn-6.0
08/21/2017 08:51:33: 		Build Branch: HEAD
08/21/2017 08:51:33: 		Build SHA1: 57260963c605c12d3796e37783433518ab8ba039
08/21/2017 08:51:33: 		Built by Source/CNTK/buildinfo.h$$0 on 516c40771854
08/21/2017 08:51:33: 		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
08/21/2017 08:51:33: 		MPI distribution: Open MPI
08/21/2017 08:51:33: 		MPI version: 1.10.7
08/21/2017 08:51:33: -------------------------------------------------------------------
08/21/2017 08:51:33: -------------------------------------------------------------------
08/21/2017 08:51:33: GPU info:

08/21/2017 08:51:33: 		Device[0]: cores = 2880; computeCapability = 3.5; type = "GeForce GTX 780 Ti"; total memory = 3020 MB; free memory = 3018 MB
08/21/2017 08:51:33: -------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: 02_BatchNormConv_ndl_deprecated.cntk:command=Train:Test
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ConfigDir=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:currentDirectory=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:DataDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:deviceId=0
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:imageLayout=cudnn
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:initOnCPUOnly=true
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ModelDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:ndlMacros=/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/../Macros.ndl
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:numMBsToShowResult=500
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:OutputDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:precision=float
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:RootDir=.
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:RunDir=/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:stderr=-
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:Test=[
    action = "test"
    modelPath = "/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv"
    minibatchSize = 16
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData/Test_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
]

configparameters: 02_BatchNormConv_ndl_deprecated.cntk:timestamping=true
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:traceLevel=1
configparameters: 02_BatchNormConv_ndl_deprecated.cntk:Train=[
    action = "train"
    modelPath = "/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv"
     NDLNetworkBuilder = [
        networkDescription = "/home/philly/jenkins/workspace/CNTK-Test-Linux-W1/Tests/EndToEndTests/Examples/Image/Deprecated/CIFAR-10/02_BatchNormConv/02_BatchNormConv.ndl"
    ]
    SGD = [
        epochSize = 49984
        minibatchSize = 64
        learningRatesPerMB = 0.03*7:0.01
        momentumPerMB = 0
        maxEpochs = 10
        L2RegWeight = 0
        dropoutRate = 0
    ]
    reader = [
        readerType = "CNTKTextFormatReader"
        file = "/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/TestData/Train_cntk_text.txt"
        input = [
            features = [
                dim = 3072
                format = "dense"
            ]
            labels = [
                dim = 10
                format = "dense"
            ]
        ]
    ]    
] [SGD=[maxEpochs=5]] [SGD=[epochSize=100]]

08/21/2017 08:51:33: Commands: Train Test
08/21/2017 08:51:33: precision = "float"

08/21/2017 08:51:33: ##############################################################################
08/21/2017 08:51:33: #                                                                            #
08/21/2017 08:51:33: # Train command (train action)                                               #
08/21/2017 08:51:33: #                                                                            #
08/21/2017 08:51:33: ##############################################################################

08/21/2017 08:51:33: 
Creating virgin network.
NDLBuilder Using GPU 0
SetGaussianRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 32, Kernel: 5 x 5 x 3, Map: 1 x 1 x 32, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 32, Output: 15 x 15 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c.c.c: using cuDNN convolution engine for geometry: Input: 15 x 15 x 32, Output: 15 x 15 x 32, Kernel: 5 x 5 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool2: using cuDNN convolution engine for geometry: Input: 15 x 15 x 32, Output: 7 x 7 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c.c.c: using cuDNN convolution engine for geometry: Input: 7 x 7 x 32, Output: 7 x 7 x 64, Kernel: 5 x 5 x 32, Map: 1 x 1 x 64, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool3: using cuDNN convolution engine for geometry: Input: 7 x 7 x 64, Output: 3 x 3 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
Using CNTK batch normalization engine.
08/21/2017 08:51:34: 
Model has 49 nodes. Using GPU 0.

08/21/2017 08:51:34: Training criterion:   CE = CrossEntropyWithSoftmax
08/21/2017 08:51:34: Evaluation criterion: Err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 4 are aliased.
	features (gradient) reuses featScaled (gradient)
	OutputNodes.t (gradient) reuses OutputNodes.z (gradient)

Memory Sharing: Out of 81 matrices, 43 are shared as 12, and 38 are not shared.

Here are the ones that share memory:
	{ conv2.c.c.b : [32 x 1] (gradient)
	  pool2 : [7 x 7 x 32 x *] }
	{ conv1.c.W : [32 x 75] (gradient)
	  conv1.c.c.y : [32 x 32 x 32 x *]
	  conv1.y : [32 x 32 x 32 x *] (gradient)
	  conv2.c.c.c : [15 x 15 x 32 x *] }
	{ conv1.c.c.c : [32 x 32 x 32 x *] (gradient)
	  conv1.y : [32 x 32 x 32 x *] }
	{ conv2.c.c.sc : [32 x 1] (gradient)
	  conv3.y : [7 x 7 x 64 x *]
	  pool2 : [7 x 7 x 32 x *] (gradient) }
	{ OutputNodes.z : [10 x *]
	  conv3.c.c.sc : [64 x 1] (gradient)
	  h1.bn : [64 x *]
	  h1.t : [64 x *] (gradient)
	  h1.y : [64 x *] (gradient) }
	{ conv3.c.c.b : [64 x 1] (gradient)
	  h1.y : [64 x *]
	  pool3 : [3 x 3 x 64 x *] (gradient) }
	{ conv1.c.c.y : [32 x 32 x 32 x *] (gradient)
	  pool1 : [15 x 15 x 32 x *] }
	{ conv3.c.W : [64 x 800] (gradient)
	  conv3.c.c.y : [7 x 7 x 64 x *]
	  conv3.c.c.y : [7 x 7 x 64 x *] (gradient)
	  pool3 : [3 x 3 x 64 x *] }
	{ conv2.c.W : [32 x 800] (gradient)
	  conv2.y : [15 x 15 x 32 x *] (gradient)
	  conv3.c.c.c : [7 x 7 x 64 x *] }
	{ conv1.c.c.b : [32 x 1] (gradient)
	  conv2.c.c.c : [15 x 15 x 32 x *] (gradient)
	  conv2.y : [15 x 15 x 32 x *] }
	{ OutputNodes.t : [10 x *]
	  OutputNodes.t : [10 x *] (gradient)
	  OutputNodes.z : [10 x *] (gradient)
	  h1.W : [64 x 3 x 3 x 64] (gradient)
	  h1.bn : [64 x *] (gradient) }
	{ conv1.c.c.sc : [32 x 1] (gradient)
	  conv2.c.c.y : [15 x 15 x 32 x *]
	  conv2.c.c.y : [15 x 15 x 32 x *] (gradient)
	  conv3.c.c.c : [7 x 7 x 64 x *] (gradient)
	  conv3.y : [7 x 7 x 64 x *] (gradient)
	  h1.t : [64 x *]
	  pool1 : [15 x 15 x 32 x *] (gradient) }

Here are the ones that don't share memory:
	{features : [32 x 32 x 3 x *]}
	{featOffs : [1 x 1]}
	{labels : [10 x *]}
	{conv1.c.W : [32 x 75]}
	{conv1.c.c.b : [32 x 1]}
	{conv1.c.c.sc : [32 x 1]}
	{conv1.c.c.m : [32 x 1]}
	{conv1.c.c.v : [32 x 1]}
	{conv1.c.c.y.run_sample_count : [1]}
	{conv2.c.W : [32 x 800]}
	{conv3.c.c.sc : [64 x 1]}
	{conv2.c.c.b : [32 x 1]}
	{conv2.c.c.sc : [32 x 1]}
	{conv2.c.c.m : [32 x 1]}
	{conv2.c.c.v : [32 x 1]}
	{conv2.c.c.y.run_sample_count : [1]}
	{conv3.c.W : [64 x 800]}
	{conv3.c.c.b : [64 x 1]}
	{conv3.c.c.m : [64 x 1]}
	{conv3.c.c.v : [64 x 1]}
	{conv3.c.c.y.run_sample_count : [1]}
	{h1.W : [64 x 3 x 3 x 64]}
	{h1.b : [64 x 1]}
	{h1.sc : [64 x 1]}
	{h1.m : [64 x 1]}
	{h1.v : [64 x 1]}
	{OutputNodes.W : [10 x 64]}
	{h1.bn.run_sample_count : [1]}
	{OutputNodes.b : [10]}
	{CE : [1]}
	{Err : [1]}
	{OutputNodes.b : [10] (gradient)}
	{CE : [1] (gradient)}
	{OutputNodes.W : [10 x 64] (gradient)}
	{h1.b : [64 x 1] (gradient)}
	{h1.sc : [64 x 1] (gradient)}
	{featScaled : [32 x 32 x 3 x *]}
	{conv1.c.c.c : [32 x 32 x 32 x *]}


08/21/2017 08:51:34: Training 117098 parameters in 14 out of 14 parameter tensors and 32 nodes with gradient:

08/21/2017 08:51:34: 	Node 'OutputNodes.W' (LearnableParameter operation) : [10 x 64]
08/21/2017 08:51:34: 	Node 'OutputNodes.b' (LearnableParameter operation) : [10]
08/21/2017 08:51:34: 	Node 'conv1.c.W' (LearnableParameter operation) : [32 x 75]
08/21/2017 08:51:34: 	Node 'conv1.c.c.b' (LearnableParameter operation) : [32 x 1]
08/21/2017 08:51:34: 	Node 'conv1.c.c.sc' (LearnableParameter operation) : [32 x 1]
08/21/2017 08:51:34: 	Node 'conv2.c.W' (LearnableParameter operation) : [32 x 800]
08/21/2017 08:51:34: 	Node 'conv2.c.c.b' (LearnableParameter operation) : [32 x 1]
08/21/2017 08:51:34: 	Node 'conv2.c.c.sc' (LearnableParameter operation) : [32 x 1]
08/21/2017 08:51:34: 	Node 'conv3.c.W' (LearnableParameter operation) : [64 x 800]
08/21/2017 08:51:34: 	Node 'conv3.c.c.b' (LearnableParameter operation) : [64 x 1]
08/21/2017 08:51:34: 	Node 'conv3.c.c.sc' (LearnableParameter operation) : [64 x 1]
08/21/2017 08:51:34: 	Node 'h1.W' (LearnableParameter operation) : [64 x 3 x 3 x 64]
08/21/2017 08:51:34: 	Node 'h1.b' (LearnableParameter operation) : [64 x 1]
08/21/2017 08:51:34: 	Node 'h1.sc' (LearnableParameter operation) : [64 x 1]

08/21/2017 08:51:34: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

08/21/2017 08:51:34: Starting Epoch 1: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/21/2017 08:51:34: Starting minibatch loop.
08/21/2017 08:51:42: Finished Epoch[ 1 of 5]: [Training] CE = 2.26618500 * 100; Err = 0.87000000 * 100; totalSamplesSeen = 100; learningRatePerSample = 0.00046874999; epochTime=7.66881s
08/21/2017 08:51:42: SGD: Saving checkpoint model '/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv.1'

08/21/2017 08:51:42: Starting Epoch 2: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/21/2017 08:51:42: Starting minibatch loop.
08/21/2017 08:51:42: Finished Epoch[ 2 of 5]: [Training] CE = 2.24375671 * 100; Err = 0.82000000 * 100; totalSamplesSeen = 200; learningRatePerSample = 0.00046874999; epochTime=0.0138318s
08/21/2017 08:51:42: SGD: Saving checkpoint model '/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv.2'

08/21/2017 08:51:42: Starting Epoch 3: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/21/2017 08:51:42: Starting minibatch loop.
08/21/2017 08:51:42: Finished Epoch[ 3 of 5]: [Training] CE = 2.21209167 * 100; Err = 0.83000000 * 100; totalSamplesSeen = 300; learningRatePerSample = 0.00046874999; epochTime=0.0144871s
08/21/2017 08:51:42: SGD: Saving checkpoint model '/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv.3'

08/21/2017 08:51:42: Starting Epoch 4: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/21/2017 08:51:42: Starting minibatch loop.
08/21/2017 08:51:42: Finished Epoch[ 4 of 5]: [Training] CE = 2.20382126 * 100; Err = 0.84000000 * 100; totalSamplesSeen = 400; learningRatePerSample = 0.00046874999; epochTime=0.0125876s
08/21/2017 08:51:42: SGD: Saving checkpoint model '/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv.4'

08/21/2017 08:51:42: Starting Epoch 5: learning rate per sample = 0.00046875  effective momentum = 0.000000  momentum as time constant = 0.0 samples

08/21/2017 08:51:42: Starting minibatch loop.
08/21/2017 08:51:42: Finished Epoch[ 5 of 5]: [Training] CE = 2.17180069 * 100; Err = 0.77000000 * 100; totalSamplesSeen = 500; learningRatePerSample = 0.00046874999; epochTime=0.0134465s
08/21/2017 08:51:42: SGD: Saving checkpoint model '/tmp/cntk-test-20170821083849.769346/Examples/Image/Deprecated/CIFAR-10_02_BatchNormConv@release_gpu/Models/02_BatchNormConv'

08/21/2017 08:51:42: Action "train" complete.


08/21/2017 08:51:42: ##############################################################################
08/21/2017 08:51:42: #                                                                            #
08/21/2017 08:51:42: # Test command (test action)                                                 #
08/21/2017 08:51:42: #                                                                            #
08/21/2017 08:51:42: ##############################################################################


Post-processing network...

3 roots:
	CE = CrossEntropyWithSoftmax()
	Err = ClassificationError()
	OutputNodes.z = Plus()

Validating network. 49 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> OutputNodes.W = LearnableParameter() :  -> [10 x 64]
Validating --> h1.W = LearnableParameter() :  -> [64 x 3 x 3 x 64]
Validating --> conv3.c.W = LearnableParameter() :  -> [64 x 800]
Validating --> conv2.c.W = LearnableParameter() :  -> [32 x 800]
Validating --> conv1.c.W = LearnableParameter() :  -> [32 x 75]
Validating --> features = InputValue() :  -> [32 x 32 x 3 x *1]
Validating --> featOffs = LearnableParameter() :  -> [1 x 1]
Validating --> featScaled = Minus (features, featOffs) : [32 x 32 x 3 x *1], [1 x 1] -> [32 x 32 x 3 x *1]
Validating --> conv1.c.c.c = Convolution (conv1.c.W, featScaled) : [32 x 75], [32 x 32 x 3 x *1] -> [32 x 32 x 32 x *1]
Validating --> conv1.c.c.sc = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.b = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.m = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.v = LearnableParameter() :  -> [32 x 1]
Validating --> conv1.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv1.c.c.y = BatchNormalization (conv1.c.c.c, conv1.c.c.sc, conv1.c.c.b, conv1.c.c.m, conv1.c.c.v, conv1.c.c.y.run_sample_count) : [32 x 32 x 32 x *1], [32 x 1], [32 x 1], [32 x 1], [32 x 1], [1] -> [32 x 32 x 32 x *1]
Validating --> conv1.y = RectifiedLinear (conv1.c.c.y) : [32 x 32 x 32 x *1] -> [32 x 32 x 32 x *1]
Validating --> pool1 = MaxPooling (conv1.y) : [32 x 32 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> conv2.c.c.c = Convolution (conv2.c.W, pool1) : [32 x 800], [15 x 15 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> conv2.c.c.sc = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.b = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.m = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.v = LearnableParameter() :  -> [32 x 1]
Validating --> conv2.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv2.c.c.y = BatchNormalization (conv2.c.c.c, conv2.c.c.sc, conv2.c.c.b, conv2.c.c.m, conv2.c.c.v, conv2.c.c.y.run_sample_count) : [15 x 15 x 32 x *1], [32 x 1], [32 x 1], [32 x 1], [32 x 1], [1] -> [15 x 15 x 32 x *1]
Validating --> conv2.y = RectifiedLinear (conv2.c.c.y) : [15 x 15 x 32 x *1] -> [15 x 15 x 32 x *1]
Validating --> pool2 = MaxPooling (conv2.y) : [15 x 15 x 32 x *1] -> [7 x 7 x 32 x *1]
Validating --> conv3.c.c.c = Convolution (conv3.c.W, pool2) : [64 x 800], [7 x 7 x 32 x *1] -> [7 x 7 x 64 x *1]
Validating --> conv3.c.c.sc = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.b = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.m = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.v = LearnableParameter() :  -> [64 x 1]
Validating --> conv3.c.c.y.run_sample_count = LearnableParameter() :  -> [1]
Validating --> conv3.c.c.y = BatchNormalization (conv3.c.c.c, conv3.c.c.sc, conv3.c.c.b, conv3.c.c.m, conv3.c.c.v, conv3.c.c.y.run_sample_count) : [7 x 7 x 64 x *1], [64 x 1], [64 x 1], [64 x 1], [64 x 1], [1] -> [7 x 7 x 64 x *1]
Validating --> conv3.y = RectifiedLinear (conv3.c.c.y) : [7 x 7 x 64 x *1] -> [7 x 7 x 64 x *1]
Validating --> pool3 = MaxPooling (conv3.y) : [7 x 7 x 64 x *1] -> [3 x 3 x 64 x *1]
Validating --> h1.t = Times (h1.W, pool3) : [64 x 3 x 3 x 64], [3 x 3 x 64 x *1] -> [64 x *1]
Validating --> h1.sc = LearnableParameter() :  -> [64 x 1]
Validating --> h1.b = LearnableParameter() :  -> [64 x 1]
Validating --> h1.m = LearnableParameter() :  -> [64 x 1]
Validating --> h1.v = LearnableParameter() :  -> [64 x 1]
Validating --> h1.bn.run_sample_count = LearnableParameter() :  -> [1]
Validating --> h1.bn = BatchNormalization (h1.t, h1.sc, h1.b, h1.m, h1.v, h1.bn.run_sample_count) : [64 x *1], [64 x 1], [64 x 1], [64 x 1], [64 x 1], [1] -> [64 x *1]
Validating --> h1.y = RectifiedLinear (h1.bn) : [64 x *1] -> [64 x *1]
Validating --> OutputNodes.t = Times (OutputNodes.W, h1.y) : [10 x 64], [64 x *1] -> [10 x *1]
Validating --> OutputNodes.b = LearnableParameter() :  -> [10]
Validating --> OutputNodes.z = Plus (OutputNodes.t, OutputNodes.b) : [10 x *1], [10] -> [10 x *1]
Validating --> CE = CrossEntropyWithSoftmax (labels, OutputNodes.z) : [10 x *1], [10 x *1] -> [1]
Validating --> Err = ClassificationError (labels, OutputNodes.z) : [10 x *1], [10 x *1] -> [1]

Validating network. 20 nodes to process in pass 2.


Validating network, final pass.

conv1.c.c.c: using cuDNN convolution engine for geometry: Input: 32 x 32 x 3, Output: 32 x 32 x 32, Kernel: 5 x 5 x 3, Map: 1 x 1 x 32, Stride: 1 x 1 x 3, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool1: using cuDNN convolution engine for geometry: Input: 32 x 32 x 32, Output: 15 x 15 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv2.c.c.c: using cuDNN convolution engine for geometry: Input: 15 x 15 x 32, Output: 15 x 15 x 32, Kernel: 5 x 5 x 32, Map: 1 x 1 x 32, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool2: using cuDNN convolution engine for geometry: Input: 15 x 15 x 32, Output: 7 x 7 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
conv3.c.c.c: using cuDNN convolution engine for geometry: Input: 7 x 7 x 32, Output: 7 x 7 x 64, Kernel: 5 x 5 x 32, Map: 1 x 1 x 64, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
Using CNTK batch normalization engine.
pool3: using cuDNN convolution engine for geometry: Input: 7 x 7 x 64, Output: 3 x 3 x 64, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1), AutoPad: (0), LowerPad: 0, UpperPad: 0.
Using CNTK batch normalization engine.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 49 matrices, 18 are shared as 3, and 31 are not shared.

Here are the ones that share memory:
	{ conv1.c.c.c : [32 x 32 x 32 x *1]
	  conv1.y : [32 x 32 x 32 x *1]
	  conv2.c.c.c : [15 x 15 x 32 x *1]
	  h1.bn : [64 x *1] }
	{ OutputNodes.t : [10 x *1]
	  conv1.c.c.y : [32 x 32 x 32 x *1]
	  conv2.y : [15 x 15 x 32 x *1]
	  conv3.c.c.c : [7 x 7 x 64 x *1]
	  conv3.y : [7 x 7 x 64 x *1]
	  h1.t : [64 x *1]
	  pool1 : [15 x 15 x 32 x *1] }
	{ OutputNodes.z : [10 x *1]
	  conv2.c.c.y : [15 x 15 x 32 x *1]
	  conv3.c.c.y : [7 x 7 x 64 x *1]
	  featScaled : [32 x 32 x 3 x *1]
	  h1.y : [64 x *1]
	  pool2 : [7 x 7 x 32 x *1]
	  pool3 : [3 x 3 x 64 x *1] }

Here are the ones that don't share memory:
	{h1.bn.run_sample_count : [1]}
	{conv2.c.c.y.run_sample_count : [1]}
	{conv3.c.c.y.run_sample_count : [1]}
	{conv1.c.c.y.run_sample_count : [1]}
	{conv3.c.c.b : [64 x 1]}
	{conv2.c.c.v : [32 x 1]}
	{conv1.c.W : [32 x 75]}
	{conv1.c.c.v : [32 x 1]}
	{conv1.c.c.sc : [32 x 1]}
	{conv2.c.c.b : [32 x 1]}
	{conv3.c.W : [64 x 800]}
	{featOffs : [1 x 1]}
	{conv2.c.W : [32 x 800]}
	{conv2.c.c.sc : [32 x 1]}
	{OutputNodes.W : [10 x 64]}
	{conv3.c.c.sc : [64 x 1]}
	{conv3.c.c.m : [64 x 1]}
	{h1.W : [64 x 3 x 3 x 64]}
	{labels : [10 x *1]}
	{conv1.c.c.m : [32 x 1]}
	{conv1.c.c.b : [32 x 1]}
	{conv2.c.c.m : [32 x 1]}
	{Err : [1]}
	{conv3.c.c.v : [64 x 1]}
	{h1.v : [64 x 1]}
	{h1.b : [64 x 1]}
	{h1.m : [64 x 1]}
	{features : [32 x 32 x 3 x *1]}
	{OutputNodes.b : [10]}
	{CE : [1]}
	{h1.sc : [64 x 1]}

08/21/2017 08:51:44: Minibatch[1-500]: Err = 0.84562500 * 8000; CE = 3.29240305 * 8000
08/21/2017 08:51:44: Minibatch[501-625]: Err = 0.84300000 * 2000; CE = 3.26490046 * 2000
08/21/2017 08:51:44: Final Results: Minibatch[1-625]: Err = 0.84510000 * 10000; CE = 3.28690253 * 10000; perplexity = 26.75984730

08/21/2017 08:51:44: Action "test" complete.

08/21/2017 08:51:44: __COMPLETED__